{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e7757a-ae32-41b6-ad85-f323d5f7c6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported featurewiz_polars 0.1.1. Use the following syntax:\n",
      " >> from featurewiz_polars import Featurewiz_MRMR, Featurewiz_MRMR_Model\n",
      " >> wiz = Featurewiz_MRMR(model_type='Classification')\n",
      " >> X_transformed, y_transformed = wiz.fit_transform(X_train, y_train)\n",
      " >> X_test_transformed = wiz.transform(X_test)\n",
      " >> print(wiz.selected_features)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from featurewiz_polars import Sulov_MRMR, Polars_DateTimeTransformer, Polars_CategoricalEncoder\n",
    "from featurewiz_polars import Polars_MissingTransformer, YTransformer, Polars_ColumnEncoder\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pdb\n",
    "import copy\n",
    "from featurewiz_polars import print_classification_metrics, print_regression_metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b65ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../../../documents/ram/data_sets/\"\n",
    "filename = \"ames_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "682085c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data... (1460, 81)\n",
      "Data dimensions (rows x cols) = 116800 dims\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_csv(datapath+filename, null_values=['NULL','NA'], try_parse_dates=True, infer_schema_length=10000, ignore_errors=True, )#.sample(1000)\n",
    "print('Loaded data...', df.shape)\n",
    "target = 'SalePrice' # Replace with your target column name\n",
    "model_type = 'Regression'\n",
    "if target not in df.columns:\n",
    "    print(f\"Error: Target column '{target}' not found in the CSV file.\")\n",
    "    exit()\n",
    "predictors = [x for x in df.columns if x!=target]\n",
    "X = df[predictors]\n",
    "y = df[target]\n",
    "print('Data dimensions (rows x cols) = %d dims' %(int(X.shape[0]*X.shape[1])))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0815591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to transform y in case of classification problems ##\n",
    "if model_type.lower() == 'classification':\n",
    "    X_pipeline = Pipeline([\n",
    "        ('datetime_transformer', Polars_DateTimeTransformer(datetime_features=[])), # Specify your datetime columns\n",
    "        ('cat_transformer', Polars_CategoricalEncoder(encoding_type='ordinal', categorical_features='auto')),\n",
    "        ('nan_transformer', Polars_MissingTransformer(strategy=\"median\")),\n",
    "        ('ytransformer', YTransformer()),\n",
    "        ])\n",
    "else:\n",
    "    X_pipeline = Pipeline([\n",
    "        ('datetime_transformer', Polars_DateTimeTransformer(datetime_features=[])), # Specify your datetime columns\n",
    "        ('cat_transformer', Polars_CategoricalEncoder(encoding_type='ordinal', categorical_features='auto')),\n",
    "        ('nan_transformer', Polars_MissingTransformer(strategy=\"median\")),\n",
    "        ])\n",
    "Y_pipeline = Pipeline([\n",
    "    ('featurewiz', Sulov_MRMR(model_type=model_type, corr_threshold=0.7, verbose=0)),\n",
    "    ])\n",
    "##    Usage missing value fillers\n",
    "feature_selection = Pipeline([\n",
    "        ('X_pipeline', X_pipeline),\n",
    "        ('Y_pipeline', Y_pipeline)\n",
    "    ])\n",
    "if model_type == 'Regression':\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=99)\n",
    "else:\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60434ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classx( TransformerMixin): # Class name \n",
    "    def __init__(self, model=model, \n",
    "            model_type=model_type, encoding_type='target', \n",
    "            imputation_strategy='mean', corr_threshold = 0.7,\n",
    "            verbose = 0):\n",
    "        self.model = model\n",
    "        self.model_type = model_type.lower()\n",
    "        self.encoding_type = encoding_type.lower()\n",
    "        self.imputation_strategy = imputation_strategy.lower()\n",
    "        self.corr_threshold = corr_threshold\n",
    "        self.feature_selection = feature_selection\n",
    "        self.y_encoder = Polars_ColumnEncoder()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.feature_selection.fit(X,y)\n",
    "        self.y_encoder.fit(y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if y is None:\n",
    "            return self.feature_selection.transform(X)\n",
    "        else:\n",
    "            Xt = self.feature_selection.transform(X)\n",
    "            if model_type.lower() == 'classification':\n",
    "                yt = self.y_encoder.transform(y)\n",
    "            else:\n",
    "                yt = y\n",
    "            return Xt, yt\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        Xt = self.transform(X)\n",
    "        if model_type.lower() == 'classification':\n",
    "            yt = self.y_encoder.transform(y)\n",
    "        else:\n",
    "            yt = y\n",
    "        return Xt, yt\n",
    "testp = Classx()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b371ea04-b913-4f5d-8bc5-0250571c2e0a",
   "metadata": {},
   "source": [
    "# Then with featurewiz polars edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403970ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: Regression\n",
      "SULOV selected Features (78): ['1stFlrSF', '2ndFlrSF', '3SsnPorch', 'Alley', 'BedroomAbvGr', 'BldgType', 'BsmtCond', 'BsmtExposure', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtFinType1', 'BsmtFinType2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtQual', 'BsmtUnfSF', 'CentralAir', 'Condition1', 'Condition2', 'Electrical', 'EnclosedPorch', 'ExterCond', 'ExterQual', 'Exterior1st', 'Exterior2nd', 'Fence', 'FireplaceQu', 'Fireplaces', 'FullBath', 'Functional', 'GarageArea', 'GarageCars', 'GarageCond', 'GarageFinish', 'GarageQual', 'GarageType', 'GarageYrBlt', 'GrLivArea', 'HalfBath', 'Heating', 'HeatingQC', 'HouseStyle', 'Id', 'KitchenAbvGr', 'KitchenQual', 'LandContour', 'LandSlope', 'LotArea', 'LotConfig', 'LotFrontage', 'LotShape', 'LowQualFinSF', 'MSSubClass', 'MSZoning', 'MasVnrArea', 'MasVnrType', 'MiscFeature', 'MiscVal', 'MoSold', 'Neighborhood', 'OpenPorchSF', 'OverallCond', 'OverallQual', 'PavedDrive', 'PoolArea', 'PoolQC', 'RoofMatl', 'RoofStyle', 'SaleCondition', 'SaleType', 'ScreenPorch', 'Street', 'TotalBsmtSF', 'Utilities', 'WoodDeckSF', 'YearBuilt', 'YearRemodAdd', 'YrSold']\n",
      "\n",
      "Recursive XGBoost selected Features (24): ['2ndFlrSF', 'BldgType', 'BsmtFinSF1', 'BsmtQual', 'CentralAir', 'Condition2', 'ExterQual', 'FireplaceQu', 'Fireplaces', 'FullBath', 'GarageCars', 'GrLivArea', 'KitchenAbvGr', 'KitchenQual', 'LotArea', 'MSSubClass', 'MSZoning', 'MasVnrArea', 'MasVnrType', 'Neighborhood', 'OverallQual', 'PoolArea', 'TotalBsmtSF', 'YearBuilt']\n",
      "<class 'polars.dataframe.frame.DataFrame'>\n",
      "<class 'polars.series.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "testp.fit(X_train, y_train)\n",
    "Xt, yt = testp.transform(X_train,y_train)\n",
    "Xtt = testp.transform(X_test)\n",
    "print(type(Xt))\n",
    "print(type(yt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ae28cf-2b5e-4c0e-adad-aadabf8126b9",
   "metadata": {},
   "source": [
    "# Model trained with the Polars selected feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "256dca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "modely = copy.deepcopy(model)\n",
    "modely.fit(Xt, yt)\n",
    "y_predy = modely.predict(Xtt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ada74b-050c-41c4-9ae2-315721b3f14a",
   "metadata": {},
   "source": [
    "# performance of featurewiz polars featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "651a5de4-eaf2-447f-b48a-6b3da79a5d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RMSE = 28089.508\n",
      "    Norm RMSE = 32%\n",
      "    MAE = 17550.220\n",
      "    WAPE = 10%, Bias = 0.6%\n",
      "    MAPE = 1079%\n",
      "    R-Squared = 90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ram\\AppData\\Local\\Temp\\ipykernel_19656\\3956388316.py:4: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  print_regression_metrics(y_test.to_pandas().ravel(), y_predy, verbose=1)\n"
     ]
    }
   ],
   "source": [
    "if model_type.lower() == 'classification':\n",
    "    print_classification_metrics(y_test.to_pandas().ravel(), y_predy, verbose=1)\n",
    "else:\n",
    "    print_regression_metrics(y_test.to_pandas().ravel(), y_predy, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e2c226b-d75a-4813-bbd6-434376df4800",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'disto' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdisto\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'disto' is not defined"
     ]
    }
   ],
   "source": [
    "disto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171a73f-f784-491b-ad33-ce2f420bcdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from scipy.stats import chi2_contingency\n",
    "from collections import defaultdict\n",
    "from polars import selectors as cs\n",
    "from typing import List, Dict\n",
    "import pdb\n",
    "class FeatureSelector:\n",
    "    def __init__(self, corr_threshold=0.7):\n",
    "        self.corr_threshold = corr_threshold\n",
    "\n",
    "    def _calculate_correlations(self, X: pl.DataFrame, features: List[str],\n",
    "                               numeric_cols: List[str], cat_cols: List[str]) -> pl.DataFrame:\n",
    "        \"\"\"Calculate correlations using optimized Polars methods\"\"\"\n",
    "        # Get numeric correlations in vectorized way\n",
    "        numeric_corrs = self._get_numeric_correlations(X, numeric_cols)\n",
    "        \n",
    "        # Get categorical correlations using existing logic\n",
    "        cat_corrs = self._get_categorical_correlations(X, features, numeric_cols, cat_cols)\n",
    "        \n",
    "        # Combine and return\n",
    "        return pl.concat([numeric_corrs, cat_corrs])\n",
    "    \n",
    "    def _get_numeric_correlations(self, X: pl.DataFrame, numeric_cols: List[str]) -> pl.DataFrame:\n",
    "        \"\"\"Vectorized numeric correlation calculation\"\"\"\n",
    "        if not numeric_cols:\n",
    "            return pl.DataFrame(schema=[\"feature_a\", \"feature_b\", \"correlation\"])\n",
    "        \n",
    "        return (\n",
    "            X.select(numeric_cols)\n",
    "            .corr()\n",
    "            .pipe(self._matrix_to_pairs)\n",
    "            .with_columns(correlation=pl.col(\"correlation\").abs())\n",
    "        )\n",
    "    \n",
    "    def _get_categorical_correlations(self, X: pl.DataFrame, features: List[str],\n",
    "                                     numeric_cols: List[str], cat_cols: List[str]) -> pl.DataFrame:\n",
    "        \"\"\"Calculate categorical correlations with optimizations\"\"\"\n",
    "        pairs = []\n",
    "        features = sorted(features)\n",
    "        \n",
    "        for f1, f2 in combinations(features, 2):\n",
    "            # Skip numeric-numeric pairs already handled\n",
    "            if {f1, f2}.issubset(numeric_cols):\n",
    "                continue\n",
    "                \n",
    "            # Skip pairs with low cardinality\n",
    "            if (X[f1].n_unique() < 2) or (X[f2].n_unique() < 2):\n",
    "                continue\n",
    "                \n",
    "            # Calculate Cramer's V\n",
    "            confusion = X.pivot(f2, index=f1, aggregate_function=\"len\").fill_null(0)\n",
    "            #chi2 = chi2_contingency(confusion.to_numpy())[0]\n",
    "            cho2 = chi2_contingency(confusion.to_numpy()[:,1:])\n",
    "            #n = X.height\n",
    "            num = X.shape[0]\n",
    "            #phi2 = chi2 / n\n",
    "            phi2 = cho2[3] / num\n",
    "            r, c = confusion.shape\n",
    "            corr = np.sqrt(phi2 / min(r-1, c-1))\n",
    "            \n",
    "            pairs.append((f1, f2, abs(corr)))\n",
    "        \n",
    "        return pl.DataFrame(pairs, schema=[\"feature_a\", \"feature_b\", \"correlation\"])\n",
    "    \n",
    "    def _adaptive_removal(self, corr_matrix: pl.DataFrame, mis_scores: Dict[str, float]) -> set:\n",
    "        \"\"\"Polars-optimized adaptive removal\"\"\"\n",
    "        max_mis = max(mis_scores.values())\n",
    "        \n",
    "        return (\n",
    "            corr_matrix\n",
    "            # Calculate MIS ratios\n",
    "            .with_columns(\n",
    "                mis_a=pl.col(\"feature_a\").map_dict(mis_scores),\n",
    "                mis_b=pl.col(\"feature_b\").map_dict(mis_scores)\n",
    "            )\n",
    "            # Identify removal candidates\n",
    "            .with_columns(\n",
    "                remove_a=pl.when(\n",
    "                    (pl.col(\"mis_a\") / pl.col(\"mis_b\") < 0.7) &\n",
    "                    (pl.col(\"mis_a\") < 0.5 * max_mis)\n",
    "                ).then(1).otherwise(0),\n",
    "                remove_b=pl.when(\n",
    "                    (pl.col(\"mis_b\") / pl.col(\"mis_a\") < 0.7) &\n",
    "                    (pl.col(\"mis_b\") < 0.5 * max_mis)\n",
    "                ).then(1).otherwise(0)\n",
    "            )\n",
    "            # Aggregate removal counts\n",
    "            .group_by(\"feature_a\").agg(pl.sum(\"remove_a\"))\n",
    "            .group_by(\"feature_b\").agg(pl.sum(\"remove_b\"))\n",
    "            .melt(value_vars=[\"feature_a\", \"feature_b\"], value_name=\"feature\")\n",
    "            .group_by(\"feature\").agg(pl.sum(\"value\"))\n",
    "            .filter(pl.col(\"value\") > 0)\n",
    "            .select(\"feature\")\n",
    "            .collect()\n",
    "            .to_series()\n",
    "            .to_set()\n",
    "        )\n",
    "    \n",
    "    def _matrix_to_pairs(self, corr_matrix: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Convert correlation matrix to feature pairs\"\"\"\n",
    "        return (\n",
    "            corr_matrix\n",
    "            .with_columns(feature_a=pl.Series(corr_matrix.columns))\n",
    "            .melt(id_vars=\"feature_a\", variable_name=\"feature_b\", value_name=\"correlation\")\n",
    "            .filter(pl.col(\"feature_a\") != pl.col(\"feature_b\"))\n",
    "            .unique()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4f22b-1266-4d42-95b3-084760bc6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Test Data Generation\n",
    "def generate_test_data(n=1000) -> pl.DataFrame:\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Highly correlated numerical features\n",
    "    base = np.random.normal(0, 1, n)\n",
    "    num1 = base + np.random.normal(0, 0.1, n)\n",
    "    num2 = base + np.random.normal(0, 0.1, n)\n",
    "    \n",
    "    # Uncorrelated numerical\n",
    "    num3 = np.random.uniform(0, 1, n)\n",
    "    \n",
    "    # Categorical features with association\n",
    "    cat1 = np.random.choice([\"A\", \"B\", \"C\"], n, p=[0.4, 0.4, 0.2])\n",
    "    cat2 = np.where(cat1 == \"A\", \"X\", np.where(cat1 == \"B\", \"Y\", \"Z\"))\n",
    "    \n",
    "    # Low cardinality categorical (should be filtered out)\n",
    "    cat3 = np.full(n, \"Constant\")\n",
    "    \n",
    "    return pl.DataFrame({\n",
    "        \"num1\": num1,\n",
    "        \"num2\": num2,\n",
    "        \"num3\": num3,\n",
    "        \"cat1\": cat1,\n",
    "        \"cat2\": cat2,\n",
    "        \"cat3\": cat3\n",
    "    })\n",
    "\n",
    "\n",
    "# Test Case 1: Basic Functionality\n",
    "def test_correlation_calculation():\n",
    "    df = generate_test_data()\n",
    "    selector = FeatureSelector(corr_threshold=0.6)\n",
    "    \n",
    "    numeric_cols = [\"num1\", \"num2\", \"num3\"]\n",
    "    cat_cols = [\"cat1\", \"cat2\", \"cat3\"]\n",
    "    features = numeric_cols + cat_cols\n",
    "    \n",
    "    # Calculate correlations\n",
    "    corr_matrix = selector._calculate_correlations(\n",
    "        X=df,\n",
    "        features=features,\n",
    "        numeric_cols=numeric_cols,\n",
    "        cat_cols=cat_cols\n",
    "    )\n",
    "    \n",
    "    # Verify expected correlations\n",
    "    high_corr_pairs = corr_matrix.filter(\n",
    "        pl.col(\"correlation\") >= selector.corr_threshold\n",
    "    )\n",
    "    \n",
    "    # num1 and num2 should be highly correlated\n",
    "    assert (\"num1\", \"num2\") in high_corr_pairs.select(\n",
    "        pl.col(\"feature_a\", \"feature_b\")\n",
    "    ).rows(), \"Missing numeric correlation\"\n",
    "    \n",
    "    # cat1 and cat2 should be associated\n",
    "    assert (\"cat1\", \"cat2\") in high_corr_pairs.select(\n",
    "        pl.col(\"feature_a\", \"feature_b\")\n",
    "    ).rows(), \"Missing categorical association\"\n",
    "    \n",
    "    # cat3 should be filtered out (low cardinality)\n",
    "    assert \"cat3\" not in corr_matrix[\"feature_a\"].to_list(), \\\n",
    "        \"Low cardinality feature not filtered\"\n",
    "    \n",
    "    print(\"Correlation calculation tests passed!\")\n",
    "\n",
    "\n",
    "# Run tests\n",
    "test_correlation_calculation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ef921-15e8-4f9a-ac8a-2af650ba5c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 2: Adaptive Removal\n",
    "def test_adaptive_removal():\n",
    "    df = generate_test_data()\n",
    "    selector = FeatureSelector()\n",
    "    \n",
    "    # Mock MIS scores (num2 > num1, cat2 > cat1)\n",
    "    mis_scores = {\n",
    "        \"num1\": 0.8,\n",
    "        \"num2\": 1.2,\n",
    "        \"num3\": 0.4,\n",
    "        \"cat1\": 0.7,\n",
    "        \"cat2\": 1.0\n",
    "    }\n",
    "    \n",
    "    corr_matrix = pl.DataFrame([\n",
    "        (\"num1\", \"num2\", 0.85),\n",
    "        (\"cat1\", \"cat2\", 0.72),\n",
    "        (\"num1\", \"num3\", 0.15)\n",
    "    ], schema=[\"feature_a\", \"feature_b\", \"correlation\"])\n",
    "    \n",
    "    to_remove = selector._adaptive_removal(corr_matrix, mis_scores)\n",
    "    \n",
    "    # num1 should be removed (correlated with better num2)\n",
    "    assert \"num1\" in to_remove, \"num1 not marked for removal\"\n",
    "    \n",
    "    # cat1 should be removed (correlated with better cat2)\n",
    "    assert \"cat1\" in to_remove, \"cat1 not marked for removal\"\n",
    "    \n",
    "    # num3 should NOT be removed (low correlation)\n",
    "    assert \"num3\" not in to_remove, \"num3 incorrectly marked\"\n",
    "    \n",
    "    print(\"Adaptive removal tests passed!\")\n",
    "\n",
    "test_adaptive_removal()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
